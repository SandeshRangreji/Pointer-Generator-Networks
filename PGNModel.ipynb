{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PGNModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandeshRangreji/Pointer-Generator-Networks/blob/main/PGNModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amd1XJ5JnmDK"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "  tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIOujJRQFsjR"
      },
      "source": [
        "pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVJ2Zjl2e13P"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GypFb5xFgL-S",
        "outputId": "8c8ec0b3-cd72-473b-be66-e1f9630fd587"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd8EHJ67go_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75258627-b24d-41c9-e8c0-a518099a903b"
      },
      "source": [
        "# unzips Glove word embeddings\n",
        "!unrar x \"/content/drive/MyDrive/Pointer Generator Networks/glove.6B.200d.rar\" -d \"/content/PGN/data/\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/Pointer Generator Networks/glove.6B.200d.rar\n",
            "\n",
            "Creating    /content/PGN                                              OK\n",
            "Creating    /content/PGN/data                                         OK\n",
            "Extracting  /content/PGN/data/glove.6B.200d.txt                          \b\b\b\b  1%\b\b\b\b  3%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  8%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 13%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 18%\b\b\b\b 20%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 25%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 30%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 35%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 40%\b\b\b\b 42%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 47%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 52%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 57%\b\b\b\b 59%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 64%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 69%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 74%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 79%\b\b\b\b 81%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 86%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 91%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 96%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5gXu4Z1lEOz"
      },
      "source": [
        "# class to handle data loading, splitting, preprocessing, tokenization\n",
        "class Data:\n",
        "\n",
        "  def __init__(self, vocab_size, oov_token):\n",
        "    # dictionary for contractions\n",
        "    self.tokenizer = Tokenizer(num_words = vocab_size, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', oov_token=oov_token)\n",
        "    self.contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "  # function to perform contractions\n",
        "  def split_contractions(self, sentence):\n",
        "    # parameters:\n",
        "    # sentence: article/summary (string)\n",
        "    # making a list of words from the article/summary\n",
        "    # return: article/summary after contractions (string)\n",
        "    li_sentence = sentence.split(' ')\n",
        "    # iterating through each word and replacing the contracted word if it is present in contraction dictionary\n",
        "    for i in range(len(li_sentence)):\n",
        "      li_sentence[i] = self.contractions.get(li_sentence[i], li_sentence[i])\n",
        "    # combining the list to form a string again\n",
        "    sentence = ' '.join(li_sentence)\n",
        "    return sentence\n",
        "\n",
        "  # function to handle preprocessing of articles and summaries\n",
        "  def preprocess(self, sentence):\n",
        "    # parameters:\n",
        "    # sentence: article or summary to be processed\n",
        "    # returns:\n",
        "    # sentence: cleaned article/summary\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    sentence = self.split_contractions(sentence)\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "    # removing trailing spaces\n",
        "    sentence = sentence.lower().strip()\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence  \n",
        "  \n",
        "  # splitting data into articles and summaries for training and testing\n",
        "  def split_data(self, dataset):\n",
        "    # parameters:\n",
        "    # dataset : tfds of cnn_dailymail dataset (bytes)\n",
        "    # returns:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (string)\n",
        "    train_articles = []\n",
        "    train_summaries = []\n",
        "    eval_articles = []\n",
        "    eval_summaries = []\n",
        "    # iterating through train dataset and storing articles and summaries seperately\n",
        "    for text in tfds.as_numpy(dataset['train']):\n",
        "      # decoding from bytes to string\n",
        "      article = self.preprocess(text['article'].decode(\"utf-8\"))\n",
        "      summaries = self.preprocess(text['highlights'].decode(\"utf-8\"))\n",
        "      train_articles.append(article)\n",
        "      train_summaries.append(summaries)\n",
        "\n",
        "    # iterating through validation dataset and storing articles and summaries seperately\n",
        "    for text in tfds.as_numpy(dataset['validation']):\n",
        "      # decoding from bytes to string\n",
        "      article = self.preprocess(text['article'].decode(\"utf-8\"))\n",
        "      summaries = self.preprocess(text['highlights'].decode(\"utf-8\"))\n",
        "      eval_articles.append(article)\n",
        "      eval_summaries.append(summaries)\n",
        "    return train_articles, train_summaries, eval_articles, eval_summaries\n",
        "\n",
        "  # function to tokenize data\n",
        "  def tokenize(self, train_articles, train_summaries, eval_articles, eval_summaries, vocab_size , embedding_dim, max_length_articles, max_length_summaries, truncating_type, padding_type, oov_token):\n",
        "    # parameters:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (string)\n",
        "    # vocab_size: size of vocabulary\n",
        "    # embedding_dim: dimensions of word embeddings\n",
        "    # max_lengths_articles: number of words in the longest article\n",
        "    # max_lengths_summaries: number of words in the longest summary\n",
        "    # truncating_type: pre/post truncatation\n",
        "    # padding_type: pro/post padding\n",
        "    # oov_token: specifies what oov_token should be used\n",
        "    # return:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (sequences)\n",
        "    # initialize tokenizer\n",
        "    # fit tokenizer on training input (vocab)\n",
        "    self.tokenizer.fit_on_texts(train_articles)\n",
        "    # get word index from tokenizer\n",
        "    word_index = self.tokenizer.word_index\n",
        "    # tokenize articles for training \n",
        "    train_articles = self.tokenizer.texts_to_sequences(train_articles)\n",
        "    train_articles = pad_sequences(train_articles ,maxlen=max_length_articles, padding=padding_type, truncating=truncating_type)\n",
        "    # tokenize summaries for training \n",
        "    eval_articles = self.tokenizer.texts_to_sequences(eval_articles)\n",
        "    eval_articles = pad_sequences(eval_articles, maxlen=max_length_articles, padding=padding_type, truncating=truncating_type)\n",
        "    # tokenize articles for eval \n",
        "    train_summaries = self.tokenizer.texts_to_sequences(train_summaries)\n",
        "    train_summaries = pad_sequences(train_summaries ,maxlen=max_length_summaries, padding=padding_type, truncating=truncating_type)\n",
        "    # tokenize summaries for eval \n",
        "    eval_summaries = self.tokenizer.texts_to_sequences(eval_summaries)\n",
        "    eval_summaries = pad_sequences(eval_summaries, maxlen=max_length_summaries, padding=padding_type, truncating=truncating_type)\n",
        "    return train_articles, train_summaries, eval_articles, eval_summaries, word_index\n",
        "\n",
        "  # function to get a dictionery which is the reverse of the word_index\n",
        "  def get_reverse_word_index(self, word_index):\n",
        "    # parameters:\n",
        "    # word_index : { word : id }\n",
        "    # returns\n",
        "    # reverse_word_index : { id : word }\n",
        "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "    return reverse_word_index\n",
        "\n",
        "  # get important metrics of the dataset that is needed to build a model\n",
        "  def get_data_metrics(self):\n",
        "    # return:\n",
        "    # average length of articles, average length of summaries, length of longest article, length of longest summary\n",
        "    # loading dataset\n",
        "    ds = tfds.load(\"cnn_dailymail\")\n",
        "    # decoding and splitting dataset into train and eval articles and summaries\n",
        "    train_articles, train_summaries, eval_articles, eval_summaries = self.split_data(ds)\n",
        "    train_article_sum = 0\n",
        "    max_article_len = 0\n",
        "    max_summary_len = 0\n",
        "    train_summaries_sum = 0\n",
        "    # iterating through dataset to count total number of words\n",
        "    for i in range(len(train_articles)):\n",
        "      # current article and summary length (no. of words)\n",
        "      article_len = len(train_articles[i].split())\n",
        "      summary_len = len(train_summaries[i].split())\n",
        "      # finding length of article with most number of words\n",
        "      if(article_len>max_article_len):\n",
        "        max_article_len = article_len\n",
        "      # finding length of summary with most number of words\n",
        "      if(summary_len>max_summary_len):\n",
        "        max_summary_len = summary_len\n",
        "      # calculating total number of words accross all articles and summary to calculate average\n",
        "      train_article_sum = train_article_sum + article_len\n",
        "      train_summaries_sum = train_summaries_sum + summary_len\n",
        "    return train_article_sum/len(train_articles), train_summaries_sum/len(train_summaries), max_article_len, max_summary_len \n",
        "\n",
        "  # function to load pretrained word embeddings and prepare them for embedding layer\n",
        "  def get_word_embeddings(self, word_index, vocab_size, embedding_dim):\n",
        "    # parameters:\n",
        "    # word_index: { word:id }\n",
        "    # vocab_size: size of vocabulary\n",
        "    # embedding_dim: dimension of word embeddings\n",
        "    embeddings_index = {}\n",
        "    # opening and reading word embeddings from file\n",
        "    with open('/content/PGN/data/glove.6B.200d.txt') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "\n",
        "    # converting word embeddings to matrix (weights for embedding layer) using word_index\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_matrix[i] = embedding_vector\n",
        "    return embeddings_index, embeddings_matrix\n",
        "\n",
        "  # function to convert list of articles and summaries to iterable, batched datasets\n",
        "  def batch_datasets(self, train_articles, train_summaries, eval_articles, eval_summaries, BATCH_SIZE):\n",
        "    # parameters:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (sequences)\n",
        "    # BATCH_SIZE: size of one batch in the dataset\n",
        "    # return:\n",
        "    # train_dataset, val_dataset: dataset objects for training and evaluation, batched according to BATCH_SIZE\n",
        "    # making a dataset object from the train articles and summaries\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_articles, train_summaries))\n",
        "    # batching training dataset\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    # making a dataset object from the evaluation articles and summaries\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((eval_articles, eval_summaries))\n",
        "    # batching evaluation dataset\n",
        "    val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "  def __call__(self, num_training_examples = 2048, vocab_size = 25000, embedding_dim = 200, max_length_articles = 2880, max_length_summaries = 1344, truncating_type='post', padding_type='post', oov_token='<OOV>', BATCH_SIZE=64):\n",
        "    # parameters:\n",
        "    # vocab_size: size of vocabulary\n",
        "    # embedding_dim: dimensions of word embeddings\n",
        "    # max_lengths_articles: number of words in the longest article\n",
        "    # max_lengths_summaries: number of words in the longest summary\n",
        "    # truncating_type: pre/post truncatation\n",
        "    # padding_type: pro/post padding\n",
        "    # oov_token: specifies what oov_token should be used\n",
        "    # returns:\n",
        "    # train_dataset, val_dataset: dataset objects for training and evaluation, batched according to BATCH_SIZE\n",
        "    # word_index : { word : id }\n",
        "    # reverse_word_index : { id : word }\n",
        "\n",
        "    # loading data in bytes from tfds\n",
        "    ds=tfds.load(\"cnn_dailymail\")\n",
        "    # splitting data into train and test sets of articles and summaries\n",
        "    train_articles, train_summaries, eval_articles, eval_summaries = self.split_data(ds)\n",
        "    # tokenizing data\n",
        "    train_articles, train_summaries, eval_articles, eval_summaries, word_index = self.tokenize(\n",
        "        train_articles[:num_training_examples], \n",
        "        train_summaries[:num_training_examples], \n",
        "        eval_articles[:2048], \n",
        "        eval_summaries[:2048],\n",
        "        vocab_size , \n",
        "        embedding_dim, \n",
        "        max_length_articles, \n",
        "        max_length_summaries, \n",
        "        truncating_type, \n",
        "        padding_type, \n",
        "        oov_token)\n",
        "    vocab_size = len(word_index)\n",
        "    # converting list of articles to train and evaluation datasets that are in batches\n",
        "    train_dataset, validation_dataset = self.batch_datasets(train_articles, train_summaries, eval_articles, eval_summaries, BATCH_SIZE)\n",
        "    return train_dataset, validation_dataset, word_index, self.get_reverse_word_index(word_index), vocab_size"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OehanrRoj_u"
      },
      "source": [
        "vocab_size = 100000\n",
        "num_examples = 2048\n",
        "embedding_dim = 200\n",
        "max_length_articles = 1024\n",
        "max_length_summaries = 128\n",
        "truncating_type ='post'\n",
        "padding_type ='post'\n",
        "oov_token = \"<OOV>\"\n",
        "BATCH_SIZE = 8\n",
        "data = Data(vocab_size, oov_token)\n",
        "# load, split, batch data\n",
        "train_dataset, val_dataset, word_index, reverse_word_index, vocab_size = data(\n",
        "        num_examples,\n",
        "        vocab_size, \n",
        "        embedding_dim, \n",
        "        max_length_articles, \n",
        "        max_length_summaries, \n",
        "        truncating_type, \n",
        "        padding_type, \n",
        "        oov_token,\n",
        "        BATCH_SIZE)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_pf0pxP0f6F",
        "outputId": "6f246d70-a7ce-4700-cbb5-8e85e3f05a8f"
      },
      "source": [
        "len(word_index)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43826"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBMPgFoEqxhD"
      },
      "source": [
        "# get word embeddings in the form of a matrix\n",
        "embeddings_index, embeddings_matrix = data.get_word_embeddings(word_index, vocab_size, embedding_dim)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzIxZw99RuAy",
        "outputId": "94f6ac87-9203-404a-a2e3-f1a197259bf7"
      },
      "source": [
        "embeddings_matrix.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43827, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3nKYCnZ7Tdw",
        "outputId": "a121aae0-b5ee-4038-9921-37e0877fc29d"
      },
      "source": [
        "# sample data from dataset\n",
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 1024]), TensorShape([16, 128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTHLyeX_q9yO"
      },
      "source": [
        "# class for Encoder model\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, embeddings_matrix):\n",
        "    # parameters:\n",
        "    # vocab_size: size of vocabulary\n",
        "    # embedding_dims: dimension of word embeddings\n",
        "    # enc_units: number of LSTM units in the encoder\n",
        "    # batch_sz: batch size of data\n",
        "    # embedding_matrix: word embeddings in the form of a matrix\n",
        "    super(Encoder, self).__init__()\n",
        "    # initializing model layers and some parameters of those layers\n",
        "    self.batch_sz = batch_sz\n",
        "    self.l1_enc_units = enc_units\n",
        "    self.l2_enc_units = enc_units//2\n",
        "    self.l3_enc_units = enc_units//4\n",
        "    # initializing embedding layer with pretrained word embeddings\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length_articles, weights=[embeddings_matrix], trainable=False)\n",
        "\n",
        "    ##________ LSTM layer in Encoder ------- ##\n",
        "    self.lstm_layer_1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.l1_enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform'))\n",
        "    \n",
        "    self.lstm_layer_2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.l2_enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform'))\n",
        "    \n",
        "    self.lstm_layer_3 = tf.keras.layers.LSTM(self.l3_enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    # self.lstm_layer_3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8,\n",
        "                                  #  return_sequences=True,\n",
        "                                  #  return_state=True,\n",
        "                                  #  recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "\n",
        "  # build encoder model\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    # print(x.shape)\n",
        "    # output_3, h3, c3 = self.lstm_layer_1(x, initial_state = hidden )\n",
        "    output_1, forward_h1, forward_c1, backward_h1, backward_c1 = self.lstm_layer_1(x, initial_state = hidden )\n",
        "    output_2, forward_h2, forward_c2, backward_h2, backward_c2 = self.lstm_layer_2(output_1)\n",
        "    output_3, h3, c3 = self.lstm_layer_3(output_2)\n",
        "    # output_3, forward_h3, forward_c3, backward_h3, backward_c3 = self.lstm_layer_3(output_2)\n",
        "    return output_3, h3, c3\n",
        "\n",
        "  # initializing weights\n",
        "  def initialize_hidden_state(self):\n",
        "    # return [tf.zeros((self.batch_sz, self.l1_enc_units)), tf.zeros((self.batch_sz, self.l1_enc_units))]\n",
        "    return [tf.zeros((self.batch_sz, self.l1_enc_units)), tf.zeros((self.batch_sz, self.l1_enc_units)), tf.zeros((self.batch_sz, self.l1_enc_units)), tf.zeros((self.batch_sz, self.l1_enc_units))]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPPhp9Rju5vx",
        "outputId": "2e086f12-c09c-4e4f-cd7d-d8ccd8facf55"
      },
      "source": [
        "## Test Encoder Stack\n",
        "\n",
        "units = 1024\n",
        "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE, embeddings_matrix)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (16, 1024, 256)\n",
            "Encoder h vecotr shape: (batch size, units) (16, 256)\n",
            "Encoder c vector shape: (batch size, units) (16, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDLkcSrGyTCK"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='bahdanau'):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention_type = attention_type\n",
        "    # Embedding Layer\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    #Final Dense layer on which softmax will be applied\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "    # Sampler\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "    # Create attention mechanism with memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, None, self.batch_sz*[max_length_articles], self.attention_type)\n",
        "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "    # Define the decoder with respect to fundamental rnn cell\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "  def build_rnn_cell(self, batch_sz):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='bahdanau'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, dec_h, dec_c = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_summaries-1])\n",
        "    return outputs, dec_h, dec_c"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liyPdtoz7Wfl",
        "outputId": "82efe5c0-34dc-47c6-b5b8-08bdc2b4775b"
      },
      "source": [
        "dec_units = units//4\n",
        "# Test decoder stack\n",
        "decoder = Decoder(vocab_size, embedding_dim, dec_units, BATCH_SIZE, 'bahdanau')\n",
        "sample_x = tf.random.uniform((BATCH_SIZE, max_length_summaries))\n",
        "decoder.attention_mechanism.setup_memory(sample_output)\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
        "\n",
        "sample_decoder_outputs, dec_h, dec_c = decoder(sample_x, initial_state)\n",
        "print(sample_x.shape)\n",
        "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n",
        "# print(\"Decoder hidden Shape: \", dec_h)\n",
        "print(\"Decoder cell Shape: \", dec_c.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 128)\n",
            "Decoder Outputs Shape:  (16, 127, 43826)\n",
            "Decoder cell Shape:  (16,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4nDlcp7zVxL"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # real shape = (BATCH_SIZE, max_length_output)\n",
        "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t2BwZwmzn8e"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "    # dec_input = targ\n",
        "    # real = targ\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\n",
        "    # Set the AttentionMechanism object with encoder_outputs\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\n",
        "    # Create AttentionWrapperState as initial_state for decoder\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "    pred, _, _ = decoder(dec_input, decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_function(real, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQIrH6hw0y_D",
        "outputId": "ca7e5b8a-135b-4e24-b834-9170a53cd817"
      },
      "source": [
        "EPOCHS = 1\n",
        "# steps_per_epoch = num_examples//BATCH_SIZE\n",
        "steps_per_epoch = 1\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  # if (epoch + 1) % 2 == 0:\n",
        "    # checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5497\n",
            "Epoch 1 Loss 4.5497\n",
            "Time taken for 1 epoch 168.8712022304535 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gI8_gCNJ_tQ"
      },
      "source": [
        "def evaluate_sentence(sentence):\n",
        "    sentence = data.preprocess(sentence)\n",
        "    inputs = data.tokenizer.texts_to_sequences([sentence])\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                          maxlen=max_length_articles,\n",
        "                                                          padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    inference_batch_size = inputs.shape[0]\n",
        "    # enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size, units))]\n",
        "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units)), tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "    e_out, e_h, e_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "    decoded_seq = np.zeros((1,max_length_summaries))\n",
        "    decoded_seq[0,0]=word_index['<start>']\n",
        "    li_dec = decoded_seq\n",
        "    decoded_seq_word = '<start>'\n",
        "\n",
        "    li=[]\n",
        "    len_pred_summary = max_length_summaries - 2\n",
        "    decoder.attention_mechanism.setup_memory(e_out)\n",
        "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [e_h, e_c], tf.float32)\n",
        "    decoded_seq, h, c = decoder(decoded_seq, decoder_initial_state)\n",
        "    print(decoded_seq[0][0].shape)\n",
        "    # decoded_new = numpy.reshape(decoded_seq, )\n",
        "    ans_arr = np.argmax(decoded_seq[0][0], axis = 1)\n",
        "    print(ans_arr.shape)\n",
        "    return ans_arr\n",
        "\n",
        "    # while decoded_seq_word !='<end>' and len_pred_summary>=0:\n",
        "        \n",
        "    #     decoded_seq, h,c = decoder(decoded_seq, decoder_initial_state)\n",
        "    #     print(decoded_seq[0][0][i].shape)\n",
        "    #     decoded_seq = np.argmax(decoded_seq[0][0][i])\n",
        "    #     print(decoded_seq)\n",
        "    #     decoded_seq_word = reverse_word_index.get(decoded_seq, 0)\n",
        "    #     # decoded_seq = np.zeros((1,64))\n",
        "    #     li_dec[0,i] = word_index.get(decoded_seq_word, 0)\n",
        "    #     decoded_seq = li_dec\n",
        "      \n",
        "    #     e_h = h\n",
        "    #     e_c = c\n",
        "    #     li.append(decoded_seq_word)\n",
        "        \n",
        "    #     len_pred_summary = len_pred_summary-1\n",
        "    #     i = i+1\n",
        "    \n",
        "    # # li.remove('<end>')\n",
        "    # out_final = \" \".join(li)\n",
        "    \n",
        "    # return out_final\n",
        "\n",
        "def summarize(sentence):\n",
        "  result = evaluate_sentence(sentence)\n",
        "  result = data.tokenizer.sequences_to_texts([result])\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puJefKaF1yT9"
      },
      "source": [
        "# def evaluate_sentence(sentence):\n",
        "#   sentence = data.preprocess(sentence)\n",
        "#   # print(sentence)\n",
        "#   # inputs = [data.tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "#   inputs = data.tokenizer.texts_to_sequences([sentence])\n",
        "#   # print(inputs)\n",
        "#   inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "#                                                           maxlen=max_length_articles,\n",
        "#                                                           padding='post')\n",
        "#   # print(inputs)\n",
        "#   inputs = tf.convert_to_tensor(inputs)\n",
        "#   inference_batch_size = inputs.shape[0]\n",
        "#   result = ''\n",
        "\n",
        "#   print(\"before encoder\")\n",
        "#   enc_start_state = [tf.zeros((inference_batch_size, 32)), tf.zeros((inference_batch_size,32)), tf.zeros((inference_batch_size, 32)), tf.zeros((inference_batch_size,32))]\n",
        "#   enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "#   print(\"out of encoder\")\n",
        "\n",
        "#   dec_h = enc_h\n",
        "#   dec_c = enc_c\n",
        "\n",
        "#   start_tokens = tf.fill([inference_batch_size], data.tokenizer.word_index['<start>'])\n",
        "#   print(start_tokens)\n",
        "#   end_token = data.tokenizer.word_index['<end>']\n",
        "\n",
        "#   # --OLD--\n",
        "#   # greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "#   # Instantiate BasicDecoder object\n",
        "#   # --OLD--\n",
        "#   # decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
        "#   # Setup Memory in decoder stack\n",
        "#   decoder.attention_mechanism.setup_memory(enc_out)\n",
        "\n",
        "#   # set decoder_initial_state\n",
        "#   decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
        "\n",
        "#   ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "#   ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "#   ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "#   decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "#   # --NEW--\n",
        "#   print(\"before decoder\")\n",
        "#   dec_seq = []\n",
        "#   dec_out = start_tokens\n",
        "#   dec_len = 0\n",
        "#   # --OLD--\n",
        "#   # outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
        "#   # --NEW--\n",
        "#   while dec_out != end_token and dec_len <= max_length_summaries:\n",
        "#     print(\"1\")\n",
        "#     print(dec_out)\n",
        "#     dec_out, dec_h, dec_c = decoder(dec_out, decoder_initial_state)\n",
        "#     print(\"2\")\n",
        "#     dec_out = np.argmax(dec_out[0.-1,:])\n",
        "#     dec_seq.append(dec_out)\n",
        "#     dec_len += 1\n",
        "    \n",
        "#   print(\"after decoder\")\n",
        "#   # --OLD--\n",
        "#   # return outputs.sample_id.numpy()\n",
        "#   return [dec_seq]\n",
        "\n",
        "# def summarize(sentence):\n",
        "#   result = evaluate_sentence(sentence)\n",
        "#   # result = data.tokenizer.sequences_to_texts(result)\n",
        "#   print('Input: %s' % (sentence))\n",
        "#   print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZgTmt3iQIZm",
        "outputId": "428482b2-4c62-46f3-8aac-6872983821ee"
      },
      "source": [
        "test_sentence = \"There are a number of job descriptions waiting for Darren Fletcher when he settles in at West Brom but the one he might not have expected is Saido Berahino’s nanny. Fletcher’s unveiling as the deadline day signing from Manchester United was almost eclipsed by the twenty-one-year-old striker, who is acquiring the habit of talking himself into trouble. Ten years Berahino’s senior, Fletcher will be expected to mentor a player who told the world this week that he wanted to play for a bigger club. Tony Pulis has advised Saido Berahino to focus on his performances at West Brom . Darren Fletcher has signed for the baggies where he will be asked to provide a role model for young players . That is off the pitch. On it, the Scotland midfielder wants to prove he is good enough to cut the mustard in the Premier League after finding starts harder and harder to come by at Old Trafford.\"\n",
        "summarize(test_sentence)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(127, 43826)\n",
            "(127,)\n",
            "Input: There are a number of job descriptions waiting for Darren Fletcher when he settles in at West Brom but the one he might not have expected is Saido Berahino’s nanny. Fletcher’s unveiling as the deadline day signing from Manchester United was almost eclipsed by the twenty-one-year-old striker, who is acquiring the habit of talking himself into trouble. Ten years Berahino’s senior, Fletcher will be expected to mentor a player who told the world this week that he wanted to play for a bigger club. Tony Pulis has advised Saido Berahino to focus on his performances at West Brom . Darren Fletcher has signed for the baggies where he will be asked to provide a role model for young players . That is off the pitch. On it, the Scotland midfielder wants to prove he is good enough to cut the mustard in the Premier League after finding starts harder and harder to come by at Old Trafford.\n",
            "Predicted translation: ['horvath the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__rdwVjtWxrV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "059985d3-92b6-4bcb-e1fe-67099c5e7891"
      },
      "source": [
        "reverse_word_index[72]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'there'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dtutYFmsf9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c75cf33f-ce6e-4bfe-ee12-ed8ce719c1e1"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuxE6iT2evgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9c9e99-a30d-4a77-9b2d-0befdff7999f"
      },
      "source": [
        "a = np.zeros((1,1))\n",
        "a[0,0] = 69\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[69.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT_3HjWcCZHC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}