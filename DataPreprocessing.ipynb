{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandeshRangreji/Pointer-Generator-Networks/blob/main/DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6Yg8QcNmOF"
      },
      "source": [
        "from google.colab import drive\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5gXu4Z1lEOz"
      },
      "source": [
        "# class to handle data loading, splitting, preprocessing, tokenization\n",
        "class Data:\n",
        "\n",
        "  def __init__(self):\n",
        "    # dictionary for contractions\n",
        "    self.contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "  # function to perform contractions\n",
        "  def split_contractions(self, sentence):\n",
        "    # parameters:\n",
        "    # sentence: article/summary (string)\n",
        "    # making a list of words from the article/summary\n",
        "    # return: article/summary after contractions (string)\n",
        "    li_sentence = sentence.split(' ')\n",
        "    # iterating through each word and replacing the contracted word if it is present in contraction dictionary\n",
        "    for i in range(len(li_sentence)):\n",
        "      li_sentence[i] = self.contractions.get(li_sentence[i], li_sentence[i])\n",
        "    # combining the list to form a string again\n",
        "    sentence = ' '.join(li_sentence)\n",
        "    return sentence\n",
        "\n",
        "  # function to handle preprocessing of articles and summaries\n",
        "  def preprocess(self, sentence):\n",
        "    # parameters:\n",
        "    # sentence: article or summary to be processed\n",
        "    # returns:\n",
        "    # sentence: cleaned article/summary\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    sentence = self.split_contractions(sentence)\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "    # removing trailing spaces\n",
        "    sentence = sentence.strip()\n",
        "    return sentence  \n",
        "  \n",
        "  # splitting data into articles and summaries for training and testing\n",
        "  def split_data(self, dataset):\n",
        "    # parameters:\n",
        "    # dataset : tfds of cnn_dailymail dataset (bytes)\n",
        "    # returns:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (string)\n",
        "    train_articles = []\n",
        "    train_summaries = []\n",
        "    eval_articles = []\n",
        "    eval_summaries = []\n",
        "    # iterating through train dataset and storing articles and summaries seperately\n",
        "    for text in tfds.as_numpy(dataset['train']):\n",
        "      # decoding from bytes to string\n",
        "      article = self.preprocess(text['article'].decode(\"utf-8\"))\n",
        "      summaries = self.preprocess(text['highlights'].decode(\"utf-8\"))\n",
        "      train_articles.append(article)\n",
        "      train_summaries.append(summaries)\n",
        "    # iterating through validation dataset and storing articles and summaries seperately\n",
        "    for text in tfds.as_numpy(dataset['validation']):\n",
        "      # decoding from bytes to string\n",
        "      article = self.preprocess(text['article'].decode(\"utf-8\"))\n",
        "      summaries = self.preprocess(text['highlights'].decode(\"utf-8\"))\n",
        "      eval_articles.append(article)\n",
        "      eval_summaries.append(summaries)\n",
        "    return train_articles, train_summaries, eval_articles, eval_summaries\n",
        "\n",
        "  # function to tokenize data\n",
        "  def tokenize(self, train_articles, train_summaries, eval_articles, eval_summaries, vocab_size , embedding_dim, max_length_articles, max_length_summaries, truncating_type, padding_type, oov_token):\n",
        "    # parameters:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (string)\n",
        "    # vocab_size: size of vocabulary\n",
        "    # embedding_dim: dimensions of word embeddings\n",
        "    # max_lengths_articles: number of words in the longest article\n",
        "    # max_lengths_summaries: number of words in the longest summary\n",
        "    # truncating_type: pre/post truncatation\n",
        "    # padding_type: pro/post padding\n",
        "    # oov_token: specifies what oov_token should be used\n",
        "    # return:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (sequences)\n",
        "    # initialize tokenizer\n",
        "    tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
        "    # fit tokenizer on training input (vocab)\n",
        "    tokenizer.fit_on_texts(train_articles)\n",
        "    # get word index from tokenizer\n",
        "    word_index = tokenizer.word_index\n",
        "    # tokenize articles for training \n",
        "    train_articles = tokenizer.texts_to_sequences(train_articles)\n",
        "    train_articles = pad_sequences(train_articles ,maxlen=max_length_articles, padding=padding_type, truncating=truncating_type)\n",
        "    # tokenize summaries for training \n",
        "    eval_articles = tokenizer.texts_to_sequences(eval_articles)\n",
        "    eval_articles = pad_sequences(eval_articles, maxlen=max_length_articles, padding=padding_type, truncating=truncating_type)\n",
        "    # tokenize articles for eval \n",
        "    train_summaries = tokenizer.texts_to_sequences(train_summaries)\n",
        "    train_summaries = pad_sequences(train_summaries ,maxlen=max_length_summaries, padding=padding_type, truncating=truncating_type)\n",
        "    # tokenize summaries for eval \n",
        "    eval_summaries = tokenizer.texts_to_sequences(eval_summaries)\n",
        "    eval_summaries = pad_sequences(eval_summaries, maxlen=max_length_summaries, padding=padding_type, truncating=truncating_type)\n",
        "    return train_articles, train_summaries, eval_articles, eval_summaries, word_index\n",
        "\n",
        "  # function to get a dictionery which is the reverse of the word_index\n",
        "  def get_reverse_word_index(self, word_index):\n",
        "    # parameters:\n",
        "    # word_index : { word : id }\n",
        "    # returns\n",
        "    # reverse_word_index : { id : word }\n",
        "    reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "    return reverse_word_index\n",
        "\n",
        "  # get important metrics of the dataset that is needed to build a model\n",
        "  def get_data_metrics(self):\n",
        "    # return:\n",
        "    # average length of articles, average length of summaries, length of longest article, length of longest summary\n",
        "    # loading dataset\n",
        "    ds = tfds.load(\"cnn_dailymail\")\n",
        "    # decoding and splitting dataset into train and eval articles and summaries\n",
        "    train_articles, train_summaries, eval_articles, eval_summaries = self.split_data(ds)\n",
        "    train_article_sum = 0\n",
        "    max_article_len = 0\n",
        "    max_summary_len = 0\n",
        "    train_summaries_sum = 0\n",
        "    # iterating through dataset to count total number of words\n",
        "    for i in range(len(train_articles)):\n",
        "      # current article and summary length (no. of words)\n",
        "      article_len = len(train_articles[i].split())\n",
        "      summary_len = len(train_summaries[i].split())\n",
        "      # finding length of article with most number of words\n",
        "      if(article_len>max_article_len):\n",
        "        max_article_len = article_len\n",
        "      # finding length of summary with most number of words\n",
        "      if(summary_len>max_summary_len):\n",
        "        max_summary_len = summary_len\n",
        "      # calculating total number of words accross all articles and summary to calculate average\n",
        "      train_article_sum = train_article_sum + article_len\n",
        "      train_summaries_sum = train_summaries_sum + summary_len\n",
        "    return train_article_sum/len(train_articles), train_summaries_sum/len(train_summaries), max_article_len, max_summary_len \n",
        "\n",
        "  # function to load pretrained word embeddings and prepare them for embedding layer\n",
        "  def get_word_embeddings(self, word_index, vocab_size, embedding_dim):\n",
        "    # parameters:\n",
        "    # word_index: { word:id }\n",
        "    # vocab_size: size of vocabulary\n",
        "    # embedding_dim: dimension of word embeddings\n",
        "    embeddings_index = {}\n",
        "    # opening and reading word embeddings from file\n",
        "    with open('/content/PGN/data/glove.6B.200d.txt') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
        "\n",
        "    # converting word embeddings to matrix (weights for embedding layer) using word_index\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_matrix[i] = embedding_vector\n",
        "    return embeddings_index, embeddings_matrix\n",
        "\n",
        "  # function to convert list of articles and summaries to iterable, batched datasets\n",
        "  def batch_datasets(self, train_articles, train_summaries, eval_articles, eval_summaries, BATCH_SIZE):\n",
        "    # parameters:\n",
        "    # train_articles, train_summaries, eval_articles, eval_summaries : lists of training and eval articles and summaries (sequences)\n",
        "    # BATCH_SIZE: size of one batch in the dataset\n",
        "    # return:\n",
        "    # train_dataset, val_dataset: dataset objects for training and evaluation, batched according to BATCH_SIZE\n",
        "    # making a dataset object from the train articles and summaries\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_articles, train_summaries))\n",
        "    # batching training dataset\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    # making a dataset object from the evaluation articles and summaries\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((eval_articles, eval_summaries))\n",
        "    # batching evaluation dataset\n",
        "    val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "  def __call__(self, vocab_size = 25000, embedding_dim = 200, max_length_articles = 2880, max_length_summaries = 1344, truncating_type='post', padding_type='post', oov_token='<OOV>', BATCH_SIZE=64):\n",
        "    # parameters:\n",
        "    # vocab_size: size of vocabulary\n",
        "    # embedding_dim: dimensions of word embeddings\n",
        "    # max_lengths_articles: number of words in the longest article\n",
        "    # max_lengths_summaries: number of words in the longest summary\n",
        "    # truncating_type: pre/post truncatation\n",
        "    # padding_type: pro/post padding\n",
        "    # oov_token: specifies what oov_token should be used\n",
        "    # returns:\n",
        "    # train_dataset, val_dataset: dataset objects for training and evaluation, batched according to BATCH_SIZE\n",
        "    # word_index : { word : id }\n",
        "    # reverse_word_index : { id : word }\n",
        "\n",
        "    # loading data in bytes from tfds\n",
        "    ds=tfds.load(\"cnn_dailymail\")\n",
        "    # splitting data into train and test sets of articles and summaries\n",
        "    train_articles, train_summaries, eval_articles, eval_summaries = self.split_data(ds)\n",
        "    # tokenizing data\n",
        "    train_articles, train_summaries, eval_articles, eval_summaries, word_index = self.tokenize(\n",
        "        train_articles[:500], \n",
        "        train_summaries[:500], \n",
        "        eval_articles[:500], \n",
        "        eval_summaries[:500],\n",
        "        vocab_size , \n",
        "        embedding_dim, \n",
        "        max_length_articles, \n",
        "        max_length_summaries, \n",
        "        truncating_type, \n",
        "        padding_type, \n",
        "        oov_token)\n",
        "    # converting list of articles to train and evaluation datasets that are in batches\n",
        "    train_dataset, validation_dataset = self.batch_datasets(train_articles, train_summaries, eval_articles, eval_summaries, BATCH_SIZE)\n",
        "    return train_dataset, validation_dataset, word_index, self.get_reverse_word_index(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OehanrRoj_u"
      },
      "source": [
        "data = Data()\n",
        "vocab_size = 25000\n",
        "embedding_dim = 200\n",
        "max_length_articles = 2880\n",
        "max_length_summaries = 1344\n",
        "truncating_type ='post'\n",
        "padding_type ='post'\n",
        "oov_token = \"<OOV>\"\n",
        "BATCH_SIZE = 64\n",
        "train_dataset, val_dataset, word_index, reverse_word_index = data(\n",
        "        vocab_size, \n",
        "        embedding_dim, \n",
        "        max_length_articles, \n",
        "        max_length_summaries, \n",
        "        truncating_type, \n",
        "        padding_type, \n",
        "        oov_token,\n",
        "        BATCH_SIZE)\n",
        "embeddings_index, embeddings_matrix = data.get_word_embeddings(word_index, vocab_size, embedding_dim)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu03HyCh3Hda"
      },
      "source": [
        "avg_train_article_words, avg_train_sum_words, max_article_len, max_summary_len = data.get_data_metrics()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIxIlqeJ3XHW",
        "outputId": "c37ad5f9-bfa3-4a7c-b613-f03ec57e6025"
      },
      "source": [
        "avg_train_article_words"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "760.9151936693914"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iA7p5VF3cBO",
        "outputId": "1cca2e92-a0a6-49b6-d2c1-4d7fa8b3a3bf"
      },
      "source": [
        "avg_train_sum_words"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53.612100462187364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qhl_W3t4vb9",
        "outputId": "e49682d4-9ed1-4ec9-b014-6b9d36073453"
      },
      "source": [
        "max_article_len"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2848"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPvFEq7r_3Ty",
        "outputId": "af821dfd-a2e5-4df1-f4f3-aac25fd42692"
      },
      "source": [
        "max_summary_len"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1319"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Qy4oiC_4jg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}